{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 랜덤 데이터 생성\n",
    "- users + items + gender + click(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>items</th>\n",
       "      <th>user</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>629</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender items user  click\n",
       "0      1    26  684      0\n",
       "1      1    56  559      0\n",
       "2      0    83  629      0\n",
       "3      0     3  192      0\n",
       "4      1    18  835      1"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSize = 30000\n",
    "NumberOfUser = 1000\n",
    "NumberOfItem = 100\n",
    "\n",
    "# random data 생성\n",
    "np.random.seed(0)\n",
    "users_ls = np.random.randint(NumberOfUser,size=sampleSize)\n",
    "users_ls_str = list(map(lambda x : str(x),users_ls))\n",
    "items_ls = np.random.randint(NumberOfItem,size=sampleSize)\n",
    "items_ls_str = list(map(lambda x : str(x),items_ls))\n",
    "gender_ls = np.random.randint(2,size=sampleSize)\n",
    "gender_ls_str = list(map(lambda x : str(x),gender_ls))\n",
    "click_ls = np.random.randint(2,size=sampleSize)\n",
    "Xdata2 = [{'user': users_ls_str[i], 'items': items_ls_str[i]} \\\n",
    "         for i in range(sampleSize)]\n",
    "Xdata3 = [{'user': users_ls_str[i], 'items': items_ls_str[i], 'gender': gender_ls_str[i]} \\\n",
    "         for i in range(sampleSize)]\n",
    "\n",
    "df = pd.DataFrame(Xdata3)\n",
    "df['click'] = click_ls\n",
    "df.head()\n",
    "# users_onehot = tf.reshape(tf.one_hot(users,depth=NumberOfUser),shape=(sampleSize,NumberOfUser))\n",
    "# items_onehot = tf.reshape(tf.one_hot(items,depth=NumberOfItem),shape=(sampleSize,NumberOfItem))\n",
    "# features = tf.concat((users_onehot, items_onehot, gender, click), axis=1)\n",
    "# print(\"features : 처음 {}칸 user 정보,그 뒤로 {}칸 item, 그 뒤로 1칸은 gender, 마지막 1칸은 click or not(label)\".format(NumberOfUser,NumberOfItem))\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = DictVectorizer()\n",
    "X_train2 = v2.fit_transform(Xdata2)\n",
    "#X_test = v.transform(test_data)\n",
    "(X_train2.toarray()[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = click_ls\n",
    "Y_train[Y_train==0]=-1  # 0 -> -1\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_train2, Y_train, test_size=0.3) # for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) pylibfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training log loss: 0.69340\n",
      "-- Epoch 2\n",
      "Training log loss: 0.69281\n",
      "-- Epoch 3\n",
      "Training log loss: 0.69238\n",
      "-- Epoch 4\n",
      "Training log loss: 0.69190\n",
      "-- Epoch 5\n",
      "Training log loss: 0.69145\n",
      "-- Epoch 6\n",
      "Training log loss: 0.69100\n",
      "-- Epoch 7\n",
      "Training log loss: 0.69060\n",
      "-- Epoch 8\n",
      "Training log loss: 0.69020\n",
      "-- Epoch 9\n",
      "Training log loss: 0.68973\n",
      "-- Epoch 10\n",
      "Training log loss: 0.68943\n",
      "-- Epoch 11\n",
      "Training log loss: 0.68907\n",
      "-- Epoch 12\n",
      "Training log loss: 0.68868\n",
      "-- Epoch 13\n",
      "Training log loss: 0.68830\n",
      "-- Epoch 14\n",
      "Training log loss: 0.68804\n",
      "-- Epoch 15\n",
      "Training log loss: 0.68772\n",
      "-- Epoch 16\n",
      "Training log loss: 0.68738\n",
      "-- Epoch 17\n",
      "Training log loss: 0.68705\n",
      "-- Epoch 18\n",
      "Training log loss: 0.68677\n",
      "-- Epoch 19\n",
      "Training log loss: 0.68648\n",
      "-- Epoch 20\n",
      "Training log loss: 0.68619\n",
      "-- Epoch 21\n",
      "Training log loss: 0.68589\n",
      "-- Epoch 22\n",
      "Training log loss: 0.68560\n",
      "-- Epoch 23\n",
      "Training log loss: 0.68533\n",
      "-- Epoch 24\n",
      "Training log loss: 0.68510\n",
      "-- Epoch 25\n",
      "Training log loss: 0.68483\n",
      "-- Epoch 26\n",
      "Training log loss: 0.68453\n",
      "-- Epoch 27\n",
      "Training log loss: 0.68427\n",
      "-- Epoch 28\n",
      "Training log loss: 0.68407\n",
      "-- Epoch 29\n",
      "Training log loss: 0.68380\n",
      "-- Epoch 30\n",
      "Training log loss: 0.68350\n",
      "-- Epoch 31\n",
      "Training log loss: 0.68325\n",
      "-- Epoch 32\n",
      "Training log loss: 0.68305\n",
      "-- Epoch 33\n",
      "Training log loss: 0.68284\n",
      "-- Epoch 34\n",
      "Training log loss: 0.68258\n",
      "-- Epoch 35\n",
      "Training log loss: 0.68237\n",
      "-- Epoch 36\n",
      "Training log loss: 0.68214\n",
      "-- Epoch 37\n",
      "Training log loss: 0.68191\n",
      "-- Epoch 38\n",
      "Training log loss: 0.68166\n",
      "-- Epoch 39\n",
      "Training log loss: 0.68143\n",
      "-- Epoch 40\n",
      "Training log loss: 0.68120\n",
      "-- Epoch 41\n",
      "Training log loss: 0.68091\n",
      "-- Epoch 42\n",
      "Training log loss: 0.68081\n",
      "-- Epoch 43\n",
      "Training log loss: 0.68057\n",
      "-- Epoch 44\n",
      "Training log loss: 0.68035\n",
      "-- Epoch 45\n",
      "Training log loss: 0.68015\n",
      "-- Epoch 46\n",
      "Training log loss: 0.67991\n",
      "-- Epoch 47\n",
      "Training log loss: 0.67974\n",
      "-- Epoch 48\n",
      "Training log loss: 0.67950\n",
      "-- Epoch 49\n",
      "Training log loss: 0.67932\n",
      "-- Epoch 50\n",
      "Training log loss: 0.67909\n",
      "-- Epoch 51\n",
      "Training log loss: 0.67891\n",
      "-- Epoch 52\n",
      "Training log loss: 0.67871\n",
      "-- Epoch 53\n",
      "Training log loss: 0.67846\n",
      "-- Epoch 54\n",
      "Training log loss: 0.67833\n",
      "-- Epoch 55\n",
      "Training log loss: 0.67811\n",
      "-- Epoch 56\n",
      "Training log loss: 0.67789\n",
      "-- Epoch 57\n",
      "Training log loss: 0.67770\n",
      "-- Epoch 58\n",
      "Training log loss: 0.67750\n",
      "-- Epoch 59\n",
      "Training log loss: 0.67734\n",
      "-- Epoch 60\n",
      "Training log loss: 0.67714\n",
      "-- Epoch 61\n",
      "Training log loss: 0.67694\n",
      "-- Epoch 62\n",
      "Training log loss: 0.67672\n",
      "-- Epoch 63\n",
      "Training log loss: 0.67657\n",
      "-- Epoch 64\n",
      "Training log loss: 0.67637\n",
      "-- Epoch 65\n",
      "Training log loss: 0.67619\n",
      "-- Epoch 66\n",
      "Training log loss: 0.67602\n",
      "-- Epoch 67\n",
      "Training log loss: 0.67581\n",
      "-- Epoch 68\n",
      "Training log loss: 0.67565\n",
      "-- Epoch 69\n",
      "Training log loss: 0.67544\n",
      "-- Epoch 70\n",
      "Training log loss: 0.67525\n",
      "-- Epoch 71\n",
      "Training log loss: 0.67505\n",
      "-- Epoch 72\n",
      "Training log loss: 0.67489\n",
      "-- Epoch 73\n",
      "Training log loss: 0.67465\n",
      "-- Epoch 74\n",
      "Training log loss: 0.67459\n",
      "-- Epoch 75\n",
      "Training log loss: 0.67438\n",
      "-- Epoch 76\n",
      "Training log loss: 0.67420\n",
      "-- Epoch 77\n",
      "Training log loss: 0.67399\n",
      "-- Epoch 78\n",
      "Training log loss: 0.67386\n",
      "-- Epoch 79\n",
      "Training log loss: 0.67365\n",
      "-- Epoch 80\n",
      "Training log loss: 0.67346\n",
      "-- Epoch 81\n",
      "Training log loss: 0.67327\n",
      "-- Epoch 82\n",
      "Training log loss: 0.67314\n",
      "-- Epoch 83\n",
      "Training log loss: 0.67297\n",
      "-- Epoch 84\n",
      "Training log loss: 0.67280\n",
      "-- Epoch 85\n",
      "Training log loss: 0.67259\n",
      "-- Epoch 86\n",
      "Training log loss: 0.67246\n",
      "-- Epoch 87\n",
      "Training log loss: 0.67225\n",
      "-- Epoch 88\n",
      "Training log loss: 0.67203\n",
      "-- Epoch 89\n",
      "Training log loss: 0.67195\n",
      "-- Epoch 90\n",
      "Training log loss: 0.67173\n",
      "-- Epoch 91\n",
      "Training log loss: 0.67159\n",
      "-- Epoch 92\n",
      "Training log loss: 0.67142\n",
      "-- Epoch 93\n",
      "Training log loss: 0.67126\n",
      "-- Epoch 94\n",
      "Training log loss: 0.67109\n",
      "-- Epoch 95\n",
      "Training log loss: 0.67092\n",
      "-- Epoch 96\n",
      "Training log loss: 0.67074\n",
      "-- Epoch 97\n",
      "Training log loss: 0.67055\n",
      "-- Epoch 98\n",
      "Training log loss: 0.67037\n",
      "-- Epoch 99\n",
      "Training log loss: 0.67023\n",
      "-- Epoch 100\n",
      "Training log loss: 0.67004\n",
      "FM MSE: 1.2441\n"
     ]
    }
   ],
   "source": [
    "# Build and train a Factorization Machine\n",
    "fm = pylibfm.FM(num_factors=10, num_iter=100, verbose=True, task='classification', initial_learning_rate=0.001, learning_rate_schedule=\"optimal\")\n",
    "fm.fit(Xtrain,Ytrain)\n",
    "\n",
    "# Evaluate\n",
    "preds = fm.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[preds < 0.5] = -1\n",
    "preds[preds >= 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM MSE: 2.017777777777778\n",
      "acc: 0.4955555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"FM MSE:\", mean_squared_error(Ytest,preds))\n",
    "print('acc:', accuracy_score(Ytest, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Fast FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.499\n",
      "auc: 0.49672211309704883\n",
      "--- 0.017866849899291992 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from fastFM import sgd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# check time\n",
    "start_time = time.time()\n",
    "\n",
    "fm = sgd.FMClassification(n_iter=10000, init_stdev=0.2, rank=4, step_size=0.2)\n",
    "fm.fit(Xtrain, Ytrain)\n",
    "Ypred = fm.predict(Xtest)\n",
    "# probability 형태로도 예측 가능\n",
    "Ypred_proba = fm.predict_proba(Xtest)\n",
    "#print(y_pred, y_pred_proba)\n",
    "print('acc:', accuracy_score(Ytest, Ypred))\n",
    "print('auc:', roc_auc_score(Ytest, Ypred_proba))\n",
    "\n",
    "# check time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) SVM polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4991111111111111\n",
      "--- 11.753458261489868 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# check time\n",
    "start_time = time.time()\n",
    "\n",
    "clf = SVC(C=1.0, kernel='poly', degree=2, gamma = 'auto')\n",
    "classifier = clf.fit(Xtrain,Ytrain)\n",
    "\n",
    "YpredSVC = classifier.predict(Xtest)\n",
    "#print(y_pred, y_pred_proba)\n",
    "print('acc:', accuracy_score(Ytest, YpredSVC))\n",
    "\n",
    "# check time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fastFM은 0.0178초 걸림.\n",
    "- SVC는 11.753초가 걸림.\n",
    "- accuracy는 동일하지만 fast FM이 압도적으로 빠름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fast FM data set -> FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastFM.datasets import make_user_item_regression\n",
    "\n",
    "# This sets up a small test dataset.\n",
    "X, y, _ = make_user_item_regression(n_user=100, n_item=100)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y) # for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 200), (10000,))"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to binary classification task.\n",
    "y_labels = np.ones_like(y)\n",
    "y_labels[y < np.mean(y)] = -1\n",
    "#print(y_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels)\n",
    "#print(X_train.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_iter : The number of iterations\n",
    "- init_stdev : the standard deviation used to initialize the model parameter and the number of hidden variables rank per feature. \n",
    "- This are the parameters that have to be specified for every solver and task. \n",
    "- The ALS solver requires in addition the regularization values for the first l2_reg_w and second order l2_reg_V interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastFM import sgd\n",
    "fm = sgd.FMClassification(n_iter=1000, init_stdev=0.1, l2_reg_w=0,\n",
    "                          l2_reg_V=0, rank=2, step_size=0.1)\n",
    "fm.fit(X_train, y_train)\n",
    "y_pred = fm.predict(X_test)\n",
    "#y_pred\n",
    "\n",
    "# probability 형태로도 예측 가능\n",
    "y_pred_proba = fm.predict_proba(X_test)\n",
    "#y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8336\n",
      "auc: 0.9223819851284363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "print('acc:', accuracy_score(y_test, y_pred))\n",
    "print('auc:', roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyFM (데이터 로드하는것 굳)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pyfm import pylibfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "def loadData(filename,path=\"ml-100k/\"):\n",
    "    data = []\n",
    "    y = []\n",
    "    users=set()\n",
    "    items=set()\n",
    "    with open(path+filename) as f:\n",
    "        for line in f:\n",
    "            (user,movieid,rating,ts)=line.split('\\t')\n",
    "            data.append({ \"user_id\": str(user), \"movie_id\": str(movieid)})\n",
    "            y.append(float(rating))\n",
    "            users.add(user)\n",
    "            items.add(movieid)\n",
    "\n",
    "    return (data, np.array(y), users, items)\n",
    "\n",
    "(train_data, y_train, train_users, train_items) = loadData(\"ua.base\")\n",
    "(test_data, y_test, test_users, test_items) = loadData(\"ua.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'user_id': '1', 'movie_id': '1'}, 5.0)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(train_data)\n",
    "X_test = v.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.50205\n",
      "-- Epoch 2\n",
      "Training MSE: 0.48544\n",
      "-- Epoch 3\n",
      "Training MSE: 0.47044\n",
      "-- Epoch 4\n",
      "Training MSE: 0.45698\n",
      "-- Epoch 5\n",
      "Training MSE: 0.44480\n",
      "-- Epoch 6\n",
      "Training MSE: 0.43381\n",
      "-- Epoch 7\n",
      "Training MSE: 0.42378\n",
      "-- Epoch 8\n",
      "Training MSE: 0.41445\n",
      "-- Epoch 9\n",
      "Training MSE: 0.40587\n",
      "-- Epoch 10\n",
      "Training MSE: 0.39774\n",
      "-- Epoch 11\n",
      "Training MSE: 0.39014\n",
      "-- Epoch 12\n",
      "Training MSE: 0.38299\n",
      "-- Epoch 13\n",
      "Training MSE: 0.37629\n",
      "-- Epoch 14\n",
      "Training MSE: 0.36992\n",
      "-- Epoch 15\n",
      "Training MSE: 0.36386\n",
      "-- Epoch 16\n",
      "Training MSE: 0.35797\n",
      "-- Epoch 17\n",
      "Training MSE: 0.35237\n",
      "-- Epoch 18\n",
      "Training MSE: 0.34699\n",
      "-- Epoch 19\n",
      "Training MSE: 0.34178\n",
      "-- Epoch 20\n",
      "Training MSE: 0.33680\n",
      "-- Epoch 21\n",
      "Training MSE: 0.33196\n",
      "-- Epoch 22\n",
      "Training MSE: 0.32727\n",
      "-- Epoch 23\n",
      "Training MSE: 0.32272\n",
      "-- Epoch 24\n",
      "Training MSE: 0.31831\n",
      "-- Epoch 25\n",
      "Training MSE: 0.31403\n",
      "-- Epoch 26\n",
      "Training MSE: 0.30985\n",
      "-- Epoch 27\n",
      "Training MSE: 0.30583\n",
      "-- Epoch 28\n",
      "Training MSE: 0.30185\n",
      "-- Epoch 29\n",
      "Training MSE: 0.29803\n",
      "-- Epoch 30\n",
      "Training MSE: 0.29427\n",
      "-- Epoch 31\n",
      "Training MSE: 0.29066\n",
      "-- Epoch 32\n",
      "Training MSE: 0.28710\n",
      "-- Epoch 33\n",
      "Training MSE: 0.28363\n",
      "-- Epoch 34\n",
      "Training MSE: 0.28023\n",
      "-- Epoch 35\n",
      "Training MSE: 0.27693\n",
      "-- Epoch 36\n",
      "Training MSE: 0.27370\n",
      "-- Epoch 37\n",
      "Training MSE: 0.27052\n",
      "-- Epoch 38\n",
      "Training MSE: 0.26747\n",
      "-- Epoch 39\n",
      "Training MSE: 0.26445\n",
      "-- Epoch 40\n",
      "Training MSE: 0.26151\n",
      "-- Epoch 41\n",
      "Training MSE: 0.25862\n",
      "-- Epoch 42\n",
      "Training MSE: 0.25580\n",
      "-- Epoch 43\n",
      "Training MSE: 0.25305\n",
      "-- Epoch 44\n",
      "Training MSE: 0.25038\n",
      "-- Epoch 45\n",
      "Training MSE: 0.24776\n",
      "-- Epoch 46\n",
      "Training MSE: 0.24518\n",
      "-- Epoch 47\n",
      "Training MSE: 0.24267\n",
      "-- Epoch 48\n",
      "Training MSE: 0.24018\n",
      "-- Epoch 49\n",
      "Training MSE: 0.23779\n",
      "-- Epoch 50\n",
      "Training MSE: 0.23543\n",
      "-- Epoch 51\n",
      "Training MSE: 0.23313\n",
      "-- Epoch 52\n",
      "Training MSE: 0.23086\n",
      "-- Epoch 53\n",
      "Training MSE: 0.22866\n",
      "-- Epoch 54\n",
      "Training MSE: 0.22649\n",
      "-- Epoch 55\n",
      "Training MSE: 0.22436\n",
      "-- Epoch 56\n",
      "Training MSE: 0.22230\n",
      "-- Epoch 57\n",
      "Training MSE: 0.22026\n",
      "-- Epoch 58\n",
      "Training MSE: 0.21827\n",
      "-- Epoch 59\n",
      "Training MSE: 0.21631\n",
      "-- Epoch 60\n",
      "Training MSE: 0.21440\n",
      "-- Epoch 61\n",
      "Training MSE: 0.21252\n",
      "-- Epoch 62\n",
      "Training MSE: 0.21067\n",
      "-- Epoch 63\n",
      "Training MSE: 0.20886\n",
      "-- Epoch 64\n",
      "Training MSE: 0.20708\n",
      "-- Epoch 65\n",
      "Training MSE: 0.20533\n",
      "-- Epoch 66\n",
      "Training MSE: 0.20358\n",
      "-- Epoch 67\n",
      "Training MSE: 0.20193\n",
      "-- Epoch 68\n",
      "Training MSE: 0.20028\n",
      "-- Epoch 69\n",
      "Training MSE: 0.19865\n",
      "-- Epoch 70\n",
      "Training MSE: 0.19705\n",
      "-- Epoch 71\n",
      "Training MSE: 0.19548\n",
      "-- Epoch 72\n",
      "Training MSE: 0.19393\n",
      "-- Epoch 73\n",
      "Training MSE: 0.19244\n",
      "-- Epoch 74\n",
      "Training MSE: 0.19096\n",
      "-- Epoch 75\n",
      "Training MSE: 0.18949\n",
      "-- Epoch 76\n",
      "Training MSE: 0.18806\n",
      "-- Epoch 77\n",
      "Training MSE: 0.18664\n",
      "-- Epoch 78\n",
      "Training MSE: 0.18525\n",
      "-- Epoch 79\n",
      "Training MSE: 0.18388\n",
      "-- Epoch 80\n",
      "Training MSE: 0.18254\n",
      "-- Epoch 81\n",
      "Training MSE: 0.18121\n",
      "-- Epoch 82\n",
      "Training MSE: 0.17991\n",
      "-- Epoch 83\n",
      "Training MSE: 0.17862\n",
      "-- Epoch 84\n",
      "Training MSE: 0.17734\n",
      "-- Epoch 85\n",
      "Training MSE: 0.17612\n",
      "-- Epoch 86\n",
      "Training MSE: 0.17489\n",
      "-- Epoch 87\n",
      "Training MSE: 0.17368\n",
      "-- Epoch 88\n",
      "Training MSE: 0.17249\n",
      "-- Epoch 89\n",
      "Training MSE: 0.17132\n",
      "-- Epoch 90\n",
      "Training MSE: 0.17017\n",
      "-- Epoch 91\n",
      "Training MSE: 0.16904\n",
      "-- Epoch 92\n",
      "Training MSE: 0.16792\n",
      "-- Epoch 93\n",
      "Training MSE: 0.16682\n",
      "-- Epoch 94\n",
      "Training MSE: 0.16574\n",
      "-- Epoch 95\n",
      "Training MSE: 0.16466\n",
      "-- Epoch 96\n",
      "Training MSE: 0.16362\n",
      "-- Epoch 97\n",
      "Training MSE: 0.16258\n",
      "-- Epoch 98\n",
      "Training MSE: 0.16156\n",
      "-- Epoch 99\n",
      "Training MSE: 0.16055\n",
      "-- Epoch 100\n",
      "Training MSE: 0.15955\n"
     ]
    }
   ],
   "source": [
    "# Build and train a Factorization Machine\n",
    "fm = pylibfm.FM(num_factors=10, num_iter=100, verbose=True, task=\"regression\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\")\n",
    "\n",
    "fm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM MSE: 0.7528\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "preds = fm.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"FM MSE: %.4f\" % mean_squared_error(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
